# âš Note
All codes are upgraded to another repository named [BlockchainSpider](https://github.com/wuzhy1ng/BlockchainSpider), 
and this repository will be deprecated in the future!


# etherscan-spider
Ethereum data spider on etherscan

## Targets

- Crawl transaction data from etherscan api

## Dependency
- Python3.5 or higher version
- Scrapy1.7 or higher version

## Download this project
```
git clone https://github.com/RagingBear/etherscan-spider.git
```

## Set seeds of crawl task
Crawling transaction data need a seed address or a seed file, 
It's highly recommend you prepared a seed file with `csv` format in `./data` folder
Just like this:`./data/seed.csv`:
```
0x000000000532b45f47779fce440748893b257865,phish-hack
0x0000000009324b6434d7766af41908e4c49ee1d7,phish-hack
0x00000000219ab540356cbb839cbe05303d7705fa,phish-hack
...
```

## Set your etherscan api token
Crawling task needs etherscan api tokens. 
You can add your api tokens in `settings.py`
```python
# etherscan apikey
APITOKENS = [
  
]
```

## Set proxy port
If you are located in China, it's necessary for you to make a proxy for visit etherscan.
I'm prepared the middleware for `SSR`, you can see at `settings.py`:
```python
DOWNLOADER_MIDDLEWARES = {
    'etherscan_spider.middlewares.EtherscanSpiderDownloaderMiddleware': 543,
}
```
For more detail of this `DownloadMiddleware`:
```python
def process_request(self, request, spider):
  request.meta['proxy'] = "http://localhost:1080"
  return None
```
You can set `SSR` port on 1080 and start spider as usual.

## Crawl transaction with a strategy
There are three kinds of strategies for you to start crawl transaction data,
including  `Random`, `BFS` and `OPICHaircut`.
You can start a spider with `BFS` strategy on console:
```
scrapy crawl bfs_tx_spider -a file=./data/seed.csv -a depth=2
```
In this way, spider will start crawl from seed address of file and the depth within 2 floors.
You can start a spider with `BFS` strategy and control the extend count equals to 300 just like this:
```
scrapy crawl bfs_tx_spider -a file=./data/seed.csv -a epa=300
```

## Reload crawl task
Some kinds of exceptions may stop your crawl task, such as network exception, out of memery and so on.
Therefore, for each seed crawled will be saved in `./data/crawled.csv` automatically for reload crawl task to avoid
start request from the seed crawled once more.You can reload your crawl task just like this:
```
scrapy crawl bfs_tx_spider -a file=./data/seed.csv -a depth=2 -a file_expect=./data/crawled.csv
```
If you wanna to start another crawl task, please remove the `crawled.csv` generated by previous crawl task in advance.

## Data storage
You can set spider output folder with `out` argument before starting spider, and the crawled data will be saved in 
`csv` file under `out` folder indicated, for example:
```shell
scrapy crawl bfs_tx_spider -a file=./data/seed.csv -a depth=2 -a out=/home/jack/tmp
```
You can find your data in `/home/jack/tmp`.
By default, `csv` file in `./data` folder with specific strategy, for example, if you start a `BFS` spider, the output folder is `./data/BFS`,
`csv` file often looks lick this:
```
hash,from,to,value,blockNumber,timeStamp,gas,gasPrice,gasUsed,...
0x94917b89296051b066db2ac572987d8ec48a88716f51291a47d50e6b1e8cc20c,0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be,0x0a0ba956038d4a66002d612648332b9c4ab7646c,500000000000000000,6026742,1532511199,21000,60000000000,21000,...
0x5c992599647f4a95919d68c4fe137612635bb9c9530b6a4572049c6197678a81,0x26b315a3dd31f4002df033b5e493c05cdbd9d36c,0x0a0ba956038d4a66002d612648332b9c4ab7646c,950209050000000000,6030488,1532565565,21000,2000000000,21000,...
...
```

## Data export
Crawl task will generate raw data with duplicate row, you can use `export` of utils to filter duplicate row:
```
python utils.py export -i ./data/BFS/ -o ./data/output
``` 
- `-i`: It means input raw data folder
- `-o`: It means output data folder


## Console options
There are many kinds of argument for you to configure spider in console, which format as:
```shell
scrapy crawl your_spider_type -a <name>=<value> [, -a <name>=<value>]
```  
including:
- `seed`:starting crawl task from a specific address, must be a address string
- `file`:starting crawl task from a address file, must be a filename
- `file_expect`:filter address to avoid start request from the seed crawled once more, must be a filename, see `Reload crawl task`
- `out`:set data storage path, see `Data storage`
- `mask`:filter some fields of output data, if you wanna to remove `input` and `gas` fields of data, please set `mask=input,gas`
- `epa`:it means `extend per address`,which controls the maximum request count from a seed, must be a integer
- `depth`:it control the maximum depth of a subgraph from a seed, only used in `BFS` strategy, must be a integer
